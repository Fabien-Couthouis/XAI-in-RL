episode_reward_max,episode_reward_min,episode_reward_mean,episode_len_mean,episodes_this_iter,policy_reward_mean,custom_metrics,num_metric_batches_dropped,info,timesteps_this_iter,done,timesteps_total,episodes_total,experiment_id,date,timestamp,training_iteration,time_this_iter_s,time_total_s,pid,hostname,node_ip,config,time_since_restore,timesteps_since_restore,iterations_since_restore
nan,nan,nan,nan,0,{},{},0,"{'num_steps_trained': 1000, 'num_steps_sampled': 1000, 'wait_time_ms': 7.626, 'apply_time_ms': 7.859, 'dispatch_time_ms': 25.012, 'learner': {}}",1000,False,1000,0,3c53a64610424e268f2864e005a4de89,2020-11-25_16-39-40,1606318780,1,51.685291051864624,51.685291051864624,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def37da0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62048>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",51.685291051864624,1000,1
nan,nan,nan,nan,0,{},{},0,"{'num_steps_trained': 3000, 'num_steps_sampled': 3000, 'wait_time_ms': 8.177, 'apply_time_ms': 8.871, 'dispatch_time_ms': 26.37, 'learner': {}}",2000,False,3000,0,3c53a64610424e268f2864e005a4de89,2020-11-25_16-39-47,1606318787,2,7.672362327575684,59.35765337944031,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62198>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def629e8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",59.35765337944031,3000,2
nan,nan,nan,nan,0,{},{},0,"{'num_steps_trained': 5000, 'num_steps_sampled': 5000, 'wait_time_ms': 8.999, 'apply_time_ms': 8.031, 'dispatch_time_ms': 25.521, 'learner': {}}",2000,False,5000,0,3c53a64610424e268f2864e005a4de89,2020-11-25_16-39-55,1606318795,3,7.5748069286346436,66.93246030807495,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73320>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73710>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",66.93246030807495,5000,3
-423.0,-1254.0,-831.3333333333334,1000.0,6,"{'agent-0': -175.0, 'agent-1': -192.0, 'agent-2': -159.83333333333334, 'agent-3': -143.0, 'agent-4': -161.5}",{},0,"{'num_steps_trained': 7000, 'num_steps_sampled': 7000, 'wait_time_ms': 61.028, 'apply_time_ms': 7.094, 'dispatch_time_ms': 27.125, 'learner': {}}",2000,False,7000,6,3c53a64610424e268f2864e005a4de89,2020-11-25_16-40-03,1606318803,4,8.103461980819702,75.03592228889465,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62e80>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62b38>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",75.03592228889465,7000,4
-423.0,-1254.0,-831.3333333333334,1000.0,0,"{'agent-0': -175.0, 'agent-1': -192.0, 'agent-2': -159.83333333333334, 'agent-3': -143.0, 'agent-4': -161.5}",{},0,"{'num_steps_trained': 9000, 'num_steps_sampled': 9000, 'wait_time_ms': 8.51, 'apply_time_ms': 6.728, 'dispatch_time_ms': 26.03, 'learner': {}}",2000,False,9000,6,3c53a64610424e268f2864e005a4de89,2020-11-25_16-40-11,1606318811,5,7.627112865447998,82.66303515434265,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def735c0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73278>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",82.66303515434265,9000,5
-423.0,-1254.0,-831.3333333333334,1000.0,0,"{'agent-0': -175.0, 'agent-1': -192.0, 'agent-2': -159.83333333333334, 'agent-3': -143.0, 'agent-4': -161.5}",{},0,"{'num_steps_trained': 11000, 'num_steps_sampled': 11000, 'wait_time_ms': 10.473, 'apply_time_ms': 7.343, 'dispatch_time_ms': 27.077, 'learner': {}}",2000,False,11000,6,3c53a64610424e268f2864e005a4de89,2020-11-25_16-40-18,1606318818,6,7.67081618309021,90.33385133743286,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def629e8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73b38>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",90.33385133743286,11000,6
312.0,-1254.0,-371.3333333333333,1000.0,6,"{'agent-0': -64.16666666666667, 'agent-1': -86.16666666666667, 'agent-2': -79.66666666666667, 'agent-3': -60.25, 'agent-4': -81.08333333333333}",{},0,"{'num_steps_trained': 13000, 'num_steps_sampled': 13000, 'wait_time_ms': 6.831, 'apply_time_ms': 7.701, 'dispatch_time_ms': 26.626, 'learner': {}}",2000,False,13000,12,3c53a64610424e268f2864e005a4de89,2020-11-25_16-40-26,1606318826,7,7.558242082595825,97.89209342002869,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73908>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def733c8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",97.89209342002869,13000,7
312.0,-1254.0,-371.3333333333333,1000.0,0,"{'agent-0': -64.16666666666667, 'agent-1': -86.16666666666667, 'agent-2': -79.66666666666667, 'agent-3': -60.25, 'agent-4': -81.08333333333333}",{},0,"{'num_steps_trained': 15000, 'num_steps_sampled': 15000, 'wait_time_ms': 7.941, 'apply_time_ms': 8.127, 'dispatch_time_ms': 28.042, 'learner': {}}",2000,False,15000,12,3c53a64610424e268f2864e005a4de89,2020-11-25_16-40-34,1606318834,8,7.65077018737793,105.54286360740662,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62b38>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73e10>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",105.54286360740662,15000,8
312.0,-1254.0,-371.3333333333333,1000.0,0,"{'agent-0': -64.16666666666667, 'agent-1': -86.16666666666667, 'agent-2': -79.66666666666667, 'agent-3': -60.25, 'agent-4': -81.08333333333333}",{},0,"{'num_steps_trained': 17000, 'num_steps_sampled': 17000, 'wait_time_ms': 9.504, 'apply_time_ms': 8.863, 'dispatch_time_ms': 25.672, 'learner': {}}",2000,False,17000,12,3c53a64610424e268f2864e005a4de89,2020-11-25_16-40-41,1606318841,9,7.586335182189941,113.12919878959656,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73390>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73048>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",113.12919878959656,17000,9
501.0,-1254.0,-124.66666666666667,1000.0,6,"{'agent-0': -3.2777777777777777, 'agent-1': -32.0, 'agent-2': -44.77777777777778, 'agent-3': -20.666666666666668, 'agent-4': -23.944444444444443}",{},0,"{'num_steps_trained': 19000, 'num_steps_sampled': 19000, 'wait_time_ms': 7.404, 'apply_time_ms': 6.838, 'dispatch_time_ms': 28.293, 'learner': {}}",2000,False,19000,18,3c53a64610424e268f2864e005a4de89,2020-11-25_16-40-49,1606318849,10,7.49588942527771,120.62508821487427,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def628d0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62cf8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",120.62508821487427,19000,10
501.0,-1254.0,-124.66666666666667,1000.0,0,"{'agent-0': -3.2777777777777777, 'agent-1': -32.0, 'agent-2': -44.77777777777778, 'agent-3': -20.666666666666668, 'agent-4': -23.944444444444443}",{},0,"{'num_steps_trained': 21000, 'num_steps_sampled': 21000, 'wait_time_ms': 6.567, 'apply_time_ms': 8.225, 'dispatch_time_ms': 27.121, 'learner': {}}",2000,False,21000,18,3c53a64610424e268f2864e005a4de89,2020-11-25_16-40-56,1606318856,11,7.585878849029541,128.2109670639038,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73828>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def736a0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",128.2109670639038,21000,11
501.0,-1254.0,-124.66666666666667,1000.0,0,"{'agent-0': -3.2777777777777777, 'agent-1': -32.0, 'agent-2': -44.77777777777778, 'agent-3': -20.666666666666668, 'agent-4': -23.944444444444443}",{},0,"{'num_steps_trained': 23000, 'num_steps_sampled': 23000, 'wait_time_ms': 7.546, 'apply_time_ms': 8.24, 'dispatch_time_ms': 27.802, 'learner': {}}",2000,False,23000,18,3c53a64610424e268f2864e005a4de89,2020-11-25_16-41-04,1606318864,12,7.595825910568237,135.80679297447205,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def626a0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73c88>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",135.80679297447205,23000,12
528.0,-1254.0,15.125,1000.0,6,"{'agent-0': 24.666666666666668, 'agent-1': -7.875, 'agent-2': -12.541666666666666, 'agent-3': 4.375, 'agent-4': 6.5}",{},0,"{'num_steps_trained': 25000, 'num_steps_sampled': 25000, 'wait_time_ms': 6.393, 'apply_time_ms': 6.831, 'dispatch_time_ms': 23.162, 'learner': {}}",2000,False,25000,24,3c53a64610424e268f2864e005a4de89,2020-11-25_16-41-11,1606318871,13,7.071021556854248,142.8778145313263,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73be0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62ef0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",142.8778145313263,25000,13
528.0,-1254.0,15.125,1000.0,0,"{'agent-0': 24.666666666666668, 'agent-1': -7.875, 'agent-2': -12.541666666666666, 'agent-3': 4.375, 'agent-4': 6.5}",{},0,"{'num_steps_trained': 27000, 'num_steps_sampled': 27000, 'wait_time_ms': 7.5, 'apply_time_ms': 7.39, 'dispatch_time_ms': 23.277, 'learner': {}}",2000,False,27000,24,3c53a64610424e268f2864e005a4de89,2020-11-25_16-41-18,1606318878,14,7.067154407501221,149.94496893882751,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73c88>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def739b0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",149.94496893882751,27000,14
574.0,-1254.0,37.48,1000.0,1,"{'agent-0': 28.2, 'agent-1': -3.2, 'agent-2': -6.44, 'agent-3': 8.04, 'agent-4': 10.88}",{},0,"{'num_steps_trained': 29000, 'num_steps_sampled': 29000, 'wait_time_ms': 7.996, 'apply_time_ms': 8.123, 'dispatch_time_ms': 25.074, 'learner': {}}",2000,False,29000,25,3c53a64610424e268f2864e005a4de89,2020-11-25_16-41-25,1606318885,15,7.173619508743286,157.1185884475708,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def732e8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73a90>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",157.1185884475708,29000,15
574.0,-1254.0,108.86666666666666,1000.0,5,"{'agent-0': 39.833333333333336, 'agent-1': 12.633333333333333, 'agent-2': 7.1, 'agent-3': 20.466666666666665, 'agent-4': 28.833333333333332}",{},0,"{'num_steps_trained': 31000, 'num_steps_sampled': 31000, 'wait_time_ms': 5.685, 'apply_time_ms': 6.766, 'dispatch_time_ms': 24.038, 'learner': {}}",2000,False,31000,30,3c53a64610424e268f2864e005a4de89,2020-11-25_16-41-33,1606318893,16,7.117578029632568,164.23616647720337,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62b00>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def626a0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",164.23616647720337,31000,16
574.0,-1254.0,108.86666666666666,1000.0,0,"{'agent-0': 39.833333333333336, 'agent-1': 12.633333333333333, 'agent-2': 7.1, 'agent-3': 20.466666666666665, 'agent-4': 28.833333333333332}",{},0,"{'num_steps_trained': 33000, 'num_steps_sampled': 33000, 'wait_time_ms': 5.908, 'apply_time_ms': 7.58, 'dispatch_time_ms': 25.807, 'learner': {}}",2000,False,33000,30,3c53a64610424e268f2864e005a4de89,2020-11-25_16-41-40,1606318900,17,7.015620946884155,171.25178742408752,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73dd8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73208>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",171.25178742408752,33000,17
574.0,-1254.0,133.09375,1000.0,2,"{'agent-0': 43.875, 'agent-1': 16.6875, 'agent-2': 14.09375, 'agent-3': 24.03125, 'agent-4': 34.40625}",{},0,"{'num_steps_trained': 35000, 'num_steps_sampled': 35000, 'wait_time_ms': 6.444, 'apply_time_ms': 7.521, 'dispatch_time_ms': 24.914, 'learner': {}}",2000,False,35000,32,3c53a64610424e268f2864e005a4de89,2020-11-25_16-41-47,1606318907,18,7.145331621170044,178.39711904525757,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62588>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8dbe0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",178.39711904525757,35000,18
574.0,-1254.0,174.41666666666666,1000.0,4,"{'agent-0': 51.083333333333336, 'agent-1': 27.055555555555557, 'agent-2': 22.944444444444443, 'agent-3': 33.75, 'agent-4': 39.583333333333336}",{},0,"{'num_steps_trained': 37000, 'num_steps_sampled': 37000, 'wait_time_ms': 4.859, 'apply_time_ms': 8.27, 'dispatch_time_ms': 22.667, 'learner': {}}",2000,False,37000,36,3c53a64610424e268f2864e005a4de89,2020-11-25_16-41-54,1606318914,19,7.016162872314453,185.41328191757202,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73be0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d1d0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",185.41328191757202,37000,19
574.0,-1254.0,174.41666666666666,1000.0,0,"{'agent-0': 51.083333333333336, 'agent-1': 27.055555555555557, 'agent-2': 22.944444444444443, 'agent-3': 33.75, 'agent-4': 39.583333333333336}",{},0,"{'num_steps_trained': 39000, 'num_steps_sampled': 39000, 'wait_time_ms': 7.947, 'apply_time_ms': 8.672, 'dispatch_time_ms': 23.382, 'learner': {}}",2000,False,39000,36,3c53a64610424e268f2864e005a4de89,2020-11-25_16-42-01,1606318921,20,7.0638508796691895,192.4771327972412,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62860>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62a20>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",192.4771327972412,39000,20
574.0,-1254.0,193.8421052631579,1000.0,2,"{'agent-0': 52.526315789473685, 'agent-1': 31.710526315789473, 'agent-2': 28.473684210526315, 'agent-3': 36.94736842105263, 'agent-4': 44.18421052631579}",{},0,"{'num_steps_trained': 41000, 'num_steps_sampled': 41000, 'wait_time_ms': 6.43, 'apply_time_ms': 8.423, 'dispatch_time_ms': 24.028, 'learner': {}}",2000,False,41000,38,3c53a64610424e268f2864e005a4de89,2020-11-25_16-42-08,1606318928,21,7.149144887924194,199.6262776851654,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def735c0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d198>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",199.6262776851654,41000,21
589.0,-1254.0,228.0,1000.0,4,"{'agent-0': 56.642857142857146, 'agent-1': 39.142857142857146, 'agent-2': 36.54761904761905, 'agent-3': 44.023809523809526, 'agent-4': 51.642857142857146}",{},0,"{'num_steps_trained': 43000, 'num_steps_sampled': 43000, 'wait_time_ms': 5.23, 'apply_time_ms': 7.415, 'dispatch_time_ms': 26.373, 'learner': {}}",2000,False,43000,42,3c53a64610424e268f2864e005a4de89,2020-11-25_16-42-15,1606318935,22,6.884439468383789,206.5107171535492,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d400>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d2e8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",206.5107171535492,43000,22
589.0,-1254.0,228.0,1000.0,0,"{'agent-0': 56.642857142857146, 'agent-1': 39.142857142857146, 'agent-2': 36.54761904761905, 'agent-3': 44.023809523809526, 'agent-4': 51.642857142857146}",{},0,"{'num_steps_trained': 45000, 'num_steps_sampled': 45000, 'wait_time_ms': 7.637, 'apply_time_ms': 7.187, 'dispatch_time_ms': 26.939, 'learner': {}}",2000,False,45000,42,3c53a64610424e268f2864e005a4de89,2020-11-25_16-42-22,1606318942,23,7.1319286823272705,213.64264583587646,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73b70>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8de10>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",213.64264583587646,45000,23
589.0,-1254.0,239.88636363636363,1000.0,2,"{'agent-0': 58.59090909090909, 'agent-1': 41.52272727272727, 'agent-2': 40.18181818181818, 'agent-3': 45.81818181818182, 'agent-4': 53.77272727272727}",{},0,"{'num_steps_trained': 47000, 'num_steps_sampled': 47000, 'wait_time_ms': 9.793, 'apply_time_ms': 7.604, 'dispatch_time_ms': 24.756, 'learner': {}}",2000,False,47000,44,3c53a64610424e268f2864e005a4de89,2020-11-25_16-42-29,1606318949,24,7.043521165847778,220.68616700172424,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d278>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d748>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",220.68616700172424,47000,24
589.0,-1254.0,256.36170212765956,1000.0,3,"{'agent-0': 60.0, 'agent-1': 44.93617021276596, 'agent-2': 44.702127659574465, 'agent-3': 49.638297872340424, 'agent-4': 57.08510638297872}",{},0,"{'num_steps_trained': 49000, 'num_steps_sampled': 49000, 'wait_time_ms': 8.937, 'apply_time_ms': 8.101, 'dispatch_time_ms': 24.545, 'learner': {}}",2000,False,49000,47,3c53a64610424e268f2864e005a4de89,2020-11-25_16-42-36,1606318956,25,7.063650846481323,227.74981784820557,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73cf8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8dcc0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",227.74981784820557,49000,25
589.0,-1254.0,262.8541666666667,1000.0,1,"{'agent-0': 60.625, 'agent-1': 46.395833333333336, 'agent-2': 46.375, 'agent-3': 50.9375, 'agent-4': 58.520833333333336}",{},0,"{'num_steps_trained': 51000, 'num_steps_sampled': 51000, 'wait_time_ms': 6.905, 'apply_time_ms': 7.516, 'dispatch_time_ms': 26.804, 'learner': {}}",2000,False,51000,48,3c53a64610424e268f2864e005a4de89,2020-11-25_16-42-43,1606318963,26,7.084798812866211,234.83461666107178,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d3c8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8dc88>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",234.83461666107178,51000,26
589.0,-1254.0,276.27450980392155,1000.0,3,"{'agent-0': 63.15686274509804, 'agent-1': 49.80392156862745, 'agent-2': 48.94117647058823, 'agent-3': 52.490196078431374, 'agent-4': 61.88235294117647}",{},0,"{'num_steps_trained': 53000, 'num_steps_sampled': 53000, 'wait_time_ms': 7.242, 'apply_time_ms': 7.838, 'dispatch_time_ms': 25.604, 'learner': {}}",2000,False,53000,51,3c53a64610424e268f2864e005a4de89,2020-11-25_16-42-51,1606318971,27,7.093713045120239,241.92832970619202,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def735c0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73b70>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",241.92832970619202,53000,27
589.0,-1254.0,286.49056603773585,1000.0,2,"{'agent-0': 65.18867924528301, 'agent-1': 51.528301886792455, 'agent-2': 51.490566037735846, 'agent-3': 54.79245283018868, 'agent-4': 63.490566037735846}",{},0,"{'num_steps_trained': 55000, 'num_steps_sampled': 55000, 'wait_time_ms': 10.579, 'apply_time_ms': 8.382, 'dispatch_time_ms': 22.248, 'learner': {}}",2000,False,55000,53,3c53a64610424e268f2864e005a4de89,2020-11-25_16-42-58,1606318978,28,7.086968660354614,249.01529836654663,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d5f8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d780>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",249.01529836654663,55000,28
589.0,-1254.0,289.5925925925926,1000.0,1,"{'agent-0': 66.0, 'agent-1': 52.870370370370374, 'agent-2': 52.5, 'agent-3': 54.425925925925924, 'agent-4': 63.7962962962963}",{},0,"{'num_steps_trained': 57000, 'num_steps_sampled': 57000, 'wait_time_ms': 8.292, 'apply_time_ms': 7.246, 'dispatch_time_ms': 23.27, 'learner': {}}",2000,False,57000,54,3c53a64610424e268f2864e005a4de89,2020-11-25_16-43-05,1606318985,29,7.096899747848511,256.11219811439514,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8de10>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d9e8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",256.11219811439514,57000,29
626.0,-1254.0,302.9298245614035,1000.0,3,"{'agent-0': 66.56140350877193, 'agent-1': 55.228070175438596, 'agent-2': 55.63157894736842, 'agent-3': 58.75438596491228, 'agent-4': 66.75438596491227}",{},0,"{'num_steps_trained': 59000, 'num_steps_sampled': 59000, 'wait_time_ms': 6.151, 'apply_time_ms': 7.897, 'dispatch_time_ms': 24.75, 'learner': {}}",2000,False,59000,57,3c53a64610424e268f2864e005a4de89,2020-11-25_16-43-12,1606318992,30,6.98486328125,263.09706139564514,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d588>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75710>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",263.09706139564514,59000,30
626.0,-1254.0,309.9830508474576,1000.0,2,"{'agent-0': 66.83050847457628, 'agent-1': 56.728813559322035, 'agent-2': 57.983050847457626, 'agent-3': 59.69491525423729, 'agent-4': 68.7457627118644}",{},0,"{'num_steps_trained': 61000, 'num_steps_sampled': 61000, 'wait_time_ms': 5.883, 'apply_time_ms': 8.141, 'dispatch_time_ms': 23.383, 'learner': {}}",2000,False,61000,59,3c53a64610424e268f2864e005a4de89,2020-11-25_16-43-19,1606318999,31,7.10217547416687,270.199236869812,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d630>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75f60>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",270.199236869812,61000,31
626.0,-1254.0,313.0,1000.0,1,"{'agent-0': 66.61666666666666, 'agent-1': 57.8, 'agent-2': 58.733333333333334, 'agent-3': 60.333333333333336, 'agent-4': 69.51666666666667}",{},0,"{'num_steps_trained': 63000, 'num_steps_sampled': 63000, 'wait_time_ms': 11.224, 'apply_time_ms': 7.637, 'dispatch_time_ms': 25.521, 'learner': {}}",2000,False,63000,60,3c53a64610424e268f2864e005a4de89,2020-11-25_16-43-26,1606319006,32,7.062344789505005,277.261581659317,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62f28>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75470>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",277.261581659317,63000,32
626.0,-1254.0,320.95238095238096,1000.0,3,"{'agent-0': 67.98412698412699, 'agent-1': 59.476190476190474, 'agent-2': 61.285714285714285, 'agent-3': 62.03174603174603, 'agent-4': 70.17460317460318}",{},0,"{'num_steps_trained': 65000, 'num_steps_sampled': 65000, 'wait_time_ms': 6.586, 'apply_time_ms': 7.943, 'dispatch_time_ms': 23.693, 'learner': {}}",2000,False,65000,63,3c53a64610424e268f2864e005a4de89,2020-11-25_16-43-33,1606319013,33,6.99194598197937,284.2535276412964,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d6a0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75b38>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",284.2535276412964,65000,33
626.0,-1254.0,325.0153846153846,1000.0,2,"{'agent-0': 68.95384615384616, 'agent-1': 61.8, 'agent-2': 60.738461538461536, 'agent-3': 62.89230769230769, 'agent-4': 70.63076923076923}",{},0,"{'num_steps_trained': 67000, 'num_steps_sampled': 67000, 'wait_time_ms': 6.942, 'apply_time_ms': 7.41, 'dispatch_time_ms': 24.677, 'learner': {}}",2000,False,67000,65,3c53a64610424e268f2864e005a4de89,2020-11-25_16-43-40,1606319020,34,7.083196401596069,291.33672404289246,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d5f8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def732b0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",291.33672404289246,67000,34
626.0,-1254.0,328.7121212121212,1000.0,1,"{'agent-0': 68.98484848484848, 'agent-1': 62.803030303030305, 'agent-2': 61.74242424242424, 'agent-3': 63.96969696969697, 'agent-4': 71.21212121212122}",{},0,"{'num_steps_trained': 69000, 'num_steps_sampled': 69000, 'wait_time_ms': 10.535, 'apply_time_ms': 7.796, 'dispatch_time_ms': 22.482, 'learner': {}}",2000,False,69000,66,3c53a64610424e268f2864e005a4de89,2020-11-25_16-43-47,1606319027,35,7.1104209423065186,298.447144985199,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62f28>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def750b8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",298.447144985199,69000,35
626.0,-1254.0,336.42028985507244,1000.0,3,"{'agent-0': 70.4927536231884, 'agent-1': 63.710144927536234, 'agent-2': 63.710144927536234, 'agent-3': 65.8840579710145, 'agent-4': 72.6231884057971}",{},0,"{'num_steps_trained': 71000, 'num_steps_sampled': 71000, 'wait_time_ms': 9.898, 'apply_time_ms': 8.759, 'dispatch_time_ms': 24.646, 'learner': {}}",2000,False,71000,69,3c53a64610424e268f2864e005a4de89,2020-11-25_16-43-55,1606319035,36,7.312082052230835,305.7592270374298,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d7b8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d208>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",305.7592270374298,71000,36
626.0,-1254.0,339.1,1000.0,1,"{'agent-0': 70.71428571428571, 'agent-1': 63.8, 'agent-2': 64.91428571428571, 'agent-3': 66.35714285714286, 'agent-4': 73.31428571428572}",{},0,"{'num_steps_trained': 73000, 'num_steps_sampled': 73000, 'wait_time_ms': 7.508, 'apply_time_ms': 7.598, 'dispatch_time_ms': 26.195, 'learner': {}}",2000,False,73000,70,3c53a64610424e268f2864e005a4de89,2020-11-25_16-44-02,1606319042,37,7.39221715927124,313.15144419670105,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def730b8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75978>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",313.15144419670105,73000,37
626.0,-1254.0,343.0833333333333,1000.0,2,"{'agent-0': 71.06944444444444, 'agent-1': 64.72222222222223, 'agent-2': 65.51388888888889, 'agent-3': 66.77777777777777, 'agent-4': 75.0}",{},0,"{'num_steps_trained': 75000, 'num_steps_sampled': 75000, 'wait_time_ms': 8.081, 'apply_time_ms': 7.994, 'dispatch_time_ms': 24.281, 'learner': {}}",2000,False,75000,72,3c53a64610424e268f2864e005a4de89,2020-11-25_16-44-09,1606319049,38,7.03689980506897,320.18834400177,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d2b0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75278>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",320.18834400177,75000,38
626.0,-1254.0,349.73333333333335,1000.0,3,"{'agent-0': 72.70666666666666, 'agent-1': 66.24, 'agent-2': 67.38666666666667, 'agent-3': 68.0, 'agent-4': 75.4}",{},0,"{'num_steps_trained': 77000, 'num_steps_sampled': 77000, 'wait_time_ms': 8.703, 'apply_time_ms': 8.084, 'dispatch_time_ms': 25.329, 'learner': {}}",2000,False,77000,75,3c53a64610424e268f2864e005a4de89,2020-11-25_16-44-16,1606319056,39,7.105043649673462,327.2933876514435,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73cc0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75128>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",327.2933876514435,77000,39
626.0,-1254.0,351.82894736842104,1000.0,1,"{'agent-0': 72.84210526315789, 'agent-1': 66.72368421052632, 'agent-2': 67.89473684210526, 'agent-3': 68.13157894736842, 'agent-4': 76.23684210526316}",{},0,"{'num_steps_trained': 79000, 'num_steps_sampled': 79000, 'wait_time_ms': 10.673, 'apply_time_ms': 7.703, 'dispatch_time_ms': 26.126, 'learner': {}}",2000,False,79000,76,3c53a64610424e268f2864e005a4de89,2020-11-25_16-44-23,1606319063,40,6.991595506668091,334.2849831581116,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d9b0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75588>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",334.2849831581116,79000,40
626.0,-1254.0,355.6923076923077,1000.0,2,"{'agent-0': 73.62820512820512, 'agent-1': 67.51282051282051, 'agent-2': 68.43589743589743, 'agent-3': 68.97435897435898, 'agent-4': 77.14102564102564}",{},0,"{'num_steps_trained': 81000, 'num_steps_sampled': 81000, 'wait_time_ms': 6.758, 'apply_time_ms': 7.81, 'dispatch_time_ms': 23.068, 'learner': {}}",2000,False,81000,78,3c53a64610424e268f2864e005a4de89,2020-11-25_16-44-30,1606319070,41,7.0297911167144775,341.31477427482605,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62b00>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d6a0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",341.31477427482605,81000,41
626.0,-1254.0,361.48148148148147,1000.0,3,"{'agent-0': 75.32098765432099, 'agent-1': 68.96296296296296, 'agent-2': 69.14814814814815, 'agent-3': 69.5925925925926, 'agent-4': 78.45679012345678}",{},0,"{'num_steps_trained': 83000, 'num_steps_sampled': 83000, 'wait_time_ms': 11.616, 'apply_time_ms': 7.696, 'dispatch_time_ms': 24.711, 'learner': {}}",2000,False,83000,81,3c53a64610424e268f2864e005a4de89,2020-11-25_16-44-37,1606319077,42,7.171142101287842,348.4859163761139,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75be0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75ac8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",348.4859163761139,83000,42
626.0,-1254.0,363.1707317073171,1000.0,1,"{'agent-0': 75.29268292682927, 'agent-1': 69.14634146341463, 'agent-2': 70.1951219512195, 'agent-3': 69.5, 'agent-4': 79.03658536585365}",{},0,"{'num_steps_trained': 85000, 'num_steps_sampled': 85000, 'wait_time_ms': 5.877, 'apply_time_ms': 7.176, 'dispatch_time_ms': 25.638, 'learner': {}}",2000,False,85000,82,3c53a64610424e268f2864e005a4de89,2020-11-25_16-44-44,1606319084,43,6.943752288818359,355.42966866493225,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62128>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d128>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",355.42966866493225,85000,43
626.0,-1254.0,366.6547619047619,1000.0,2,"{'agent-0': 76.05952380952381, 'agent-1': 70.02380952380952, 'agent-2': 70.28571428571429, 'agent-3': 70.29761904761905, 'agent-4': 79.98809523809524}",{},0,"{'num_steps_trained': 87000, 'num_steps_sampled': 87000, 'wait_time_ms': 8.216, 'apply_time_ms': 8.232, 'dispatch_time_ms': 24.977, 'learner': {}}",2000,False,87000,84,3c53a64610424e268f2864e005a4de89,2020-11-25_16-44-51,1606319091,44,7.018342018127441,362.4480106830597,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75a20>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75160>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",362.4480106830597,87000,44
626.0,-1254.0,370.67816091954023,1000.0,3,"{'agent-0': 77.06896551724138, 'agent-1': 71.86206896551724, 'agent-2': 70.98850574712644, 'agent-3': 70.39080459770115, 'agent-4': 80.36781609195403}",{},0,"{'num_steps_trained': 89000, 'num_steps_sampled': 89000, 'wait_time_ms': 6.406, 'apply_time_ms': 8.115, 'dispatch_time_ms': 23.62, 'learner': {}}",2000,False,89000,87,3c53a64610424e268f2864e005a4de89,2020-11-25_16-44-59,1606319099,45,7.0303709506988525,369.47838163375854,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62f28>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def757f0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",369.47838163375854,89000,45
626.0,-1254.0,371.95454545454544,1000.0,1,"{'agent-0': 77.32954545454545, 'agent-1': 72.54545454545455, 'agent-2': 70.7840909090909, 'agent-3': 70.93181818181819, 'agent-4': 80.36363636363636}",{},0,"{'num_steps_trained': 91000, 'num_steps_sampled': 91000, 'wait_time_ms': 6.754, 'apply_time_ms': 8.461, 'dispatch_time_ms': 25.557, 'learner': {}}",2000,False,91000,88,3c53a64610424e268f2864e005a4de89,2020-11-25_16-45-06,1606319106,46,7.08517599105835,376.5635576248169,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def759e8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75cf8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",376.5635576248169,91000,46
626.0,-1254.0,375.3888888888889,1000.0,2,"{'agent-0': 77.97777777777777, 'agent-1': 72.81111111111112, 'agent-2': 71.36666666666666, 'agent-3': 72.1, 'agent-4': 81.13333333333334}",{},0,"{'num_steps_trained': 93000, 'num_steps_sampled': 93000, 'wait_time_ms': 6.202, 'apply_time_ms': 7.659, 'dispatch_time_ms': 24.228, 'learner': {}}",2000,False,93000,90,3c53a64610424e268f2864e005a4de89,2020-11-25_16-45-13,1606319113,47,7.3386218547821045,383.902179479599,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def626d8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73cc0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",383.902179479599,93000,47
626.0,-1254.0,379.73118279569894,1000.0,3,"{'agent-0': 79.23655913978494, 'agent-1': 73.18279569892474, 'agent-2': 72.44086021505376, 'agent-3': 72.94623655913979, 'agent-4': 81.9247311827957}",{},0,"{'num_steps_trained': 95000, 'num_steps_sampled': 95000, 'wait_time_ms': 8.771, 'apply_time_ms': 7.722, 'dispatch_time_ms': 23.896, 'learner': {}}",2000,False,95000,93,3c53a64610424e268f2864e005a4de89,2020-11-25_16-45-20,1606319120,48,7.019093990325928,390.9212734699249,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75940>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62668>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",390.9212734699249,95000,48
626.0,-1254.0,380.75531914893617,1000.0,1,"{'agent-0': 79.69148936170212, 'agent-1': 73.31914893617021, 'agent-2': 72.72340425531915, 'agent-3': 72.85106382978724, 'agent-4': 82.17021276595744}",{},0,"{'num_steps_trained': 97000, 'num_steps_sampled': 97000, 'wait_time_ms': 7.172, 'apply_time_ms': 8.685, 'dispatch_time_ms': 27.286, 'learner': {}}",2000,False,97000,94,3c53a64610424e268f2864e005a4de89,2020-11-25_16-45-27,1606319127,49,7.1043195724487305,398.02559304237366,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62b00>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73780>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",398.02559304237366,97000,49
626.0,-1254.0,383.59375,1000.0,2,"{'agent-0': 80.28125, 'agent-1': 73.5625, 'agent-2': 74.20833333333333, 'agent-3': 73.0, 'agent-4': 82.54166666666667}",{},0,"{'num_steps_trained': 99000, 'num_steps_sampled': 99000, 'wait_time_ms': 7.06, 'apply_time_ms': 7.691, 'dispatch_time_ms': 25.68, 'learner': {}}",2000,False,99000,96,3c53a64610424e268f2864e005a4de89,2020-11-25_16-45-34,1606319134,50,7.149889945983887,405.17548298835754,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75748>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def754a8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",405.17548298835754,99000,50
626.0,-1254.0,387.8484848484849,1000.0,3,"{'agent-0': 80.76767676767676, 'agent-1': 74.68686868686869, 'agent-2': 75.0909090909091, 'agent-3': 73.95959595959596, 'agent-4': 83.34343434343434}",{},0,"{'num_steps_trained': 101000, 'num_steps_sampled': 101000, 'wait_time_ms': 7.599, 'apply_time_ms': 7.743, 'dispatch_time_ms': 24.683, 'learner': {}}",2000,False,101000,99,3c53a64610424e268f2864e005a4de89,2020-11-25_16-45-41,1606319141,51,7.072674036026001,412.24815702438354,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def757b8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def625f8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",412.24815702438354,101000,51
626.0,-1254.0,389.27,1000.0,1,"{'agent-0': 80.54, 'agent-1': 74.84, 'agent-2': 75.56, 'agent-3': 74.45, 'agent-4': 83.88}",{},0,"{'num_steps_trained': 103000, 'num_steps_sampled': 103000, 'wait_time_ms': 6.541, 'apply_time_ms': 8.645, 'dispatch_time_ms': 24.393, 'learner': {}}",2000,False,103000,100,3c53a64610424e268f2864e005a4de89,2020-11-25_16-45-48,1606319148,52,7.030829191207886,419.27898621559143,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def756d8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75e10>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",419.27898621559143,103000,52
626.0,-1254.0,412.25,1000.0,2,"{'agent-0': 86.24, 'agent-1': 77.69, 'agent-2': 81.98, 'agent-3': 78.31, 'agent-4': 88.03}",{},0,"{'num_steps_trained': 105000, 'num_steps_sampled': 105000, 'wait_time_ms': 8.743, 'apply_time_ms': 8.301, 'dispatch_time_ms': 24.763, 'learner': {}}",2000,False,105000,102,3c53a64610424e268f2864e005a4de89,2020-11-25_16-45-56,1606319156,53,7.205090045928955,426.4840762615204,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73a90>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62518>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",426.4840762615204,105000,53
626.0,-961.0,457.49,1000.0,3,"{'agent-0': 95.79, 'agent-1': 87.51, 'agent-2': 89.18, 'agent-3': 85.02, 'agent-4': 99.99}",{},0,"{'num_steps_trained': 107000, 'num_steps_sampled': 107000, 'wait_time_ms': 6.426, 'apply_time_ms': 7.471, 'dispatch_time_ms': 24.881, 'learner': {}}",2000,False,107000,105,3c53a64610424e268f2864e005a4de89,2020-11-25_16-46-03,1606319163,54,7.070524215698242,433.5546004772186,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75da0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75e48>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",433.5546004772186,107000,54
626.0,-240.0,472.44,1000.0,1,"{'agent-0': 96.58, 'agent-1': 91.93, 'agent-2': 92.12, 'agent-3': 90.57, 'agent-4': 101.24}",{},0,"{'num_steps_trained': 109000, 'num_steps_sampled': 109000, 'wait_time_ms': 6.819, 'apply_time_ms': 7.73, 'dispatch_time_ms': 23.686, 'learner': {}}",2000,False,109000,106,3c53a64610424e268f2864e005a4de89,2020-11-25_16-46-10,1606319170,55,7.119722127914429,440.67432260513306,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73780>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73ba8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",440.67432260513306,109000,55
626.0,-240.0,474.77,1000.0,1,"{'agent-0': 96.54, 'agent-1': 92.54, 'agent-2': 92.27, 'agent-3': 91.06, 'agent-4': 102.36}",{},0,"{'num_steps_trained': 111000, 'num_steps_sampled': 111000, 'wait_time_ms': 7.912, 'apply_time_ms': 8.244, 'dispatch_time_ms': 25.723, 'learner': {}}",2000,False,111000,107,3c53a64610424e268f2864e005a4de89,2020-11-25_16-46-17,1606319177,56,7.067321300506592,447.74164390563965,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75518>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def756d8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",447.74164390563965,111000,56
626.0,-33.0,492.33,1000.0,4,"{'agent-0': 98.64, 'agent-1': 94.65, 'agent-2': 97.23, 'agent-3': 94.85, 'agent-4': 106.96}",{},0,"{'num_steps_trained': 113000, 'num_steps_sampled': 113000, 'wait_time_ms': 7.804, 'apply_time_ms': 8.514, 'dispatch_time_ms': 24.936, 'learner': {}}",2000,False,113000,111,3c53a64610424e268f2864e005a4de89,2020-11-25_16-46-24,1606319184,57,7.048387289047241,454.7900311946869,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62470>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62cc0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",454.7900311946869,113000,57
626.0,281.0,497.25,1000.0,1,"{'agent-0': 99.29, 'agent-1': 96.66, 'agent-2': 98.55, 'agent-3': 94.53, 'agent-4': 108.22}",{},0,"{'num_steps_trained': 115000, 'num_steps_sampled': 115000, 'wait_time_ms': 7.877, 'apply_time_ms': 7.166, 'dispatch_time_ms': 24.914, 'learner': {}}",2000,False,115000,112,3c53a64610424e268f2864e005a4de89,2020-11-25_16-46-31,1606319191,58,7.020776033401489,461.8108072280884,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75ac8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62b70>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",461.8108072280884,115000,58
626.0,308.0,499.16,1000.0,1,"{'agent-0': 99.54, 'agent-1': 96.97, 'agent-2': 99.38, 'agent-3': 94.94, 'agent-4': 108.33}",{},0,"{'num_steps_trained': 117000, 'num_steps_sampled': 117000, 'wait_time_ms': 6.983, 'apply_time_ms': 8.366, 'dispatch_time_ms': 24.278, 'learner': {}}",2000,False,117000,113,3c53a64610424e268f2864e005a4de89,2020-11-25_16-46-38,1606319198,59,7.094897031784058,468.90570425987244,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62f28>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62e10>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",468.90570425987244,117000,59
626.0,319.0,503.97,1000.0,4,"{'agent-0': 97.78, 'agent-1': 98.1, 'agent-2': 102.43, 'agent-3': 96.29, 'agent-4': 109.37}",{},0,"{'num_steps_trained': 119000, 'num_steps_sampled': 119000, 'wait_time_ms': 6.931, 'apply_time_ms': 8.23, 'dispatch_time_ms': 24.429, 'learner': {}}",2000,False,119000,117,3c53a64610424e268f2864e005a4de89,2020-11-25_16-46-45,1606319205,60,7.09907865524292,476.00478291511536,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75518>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75cc0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",476.00478291511536,119000,60
626.0,319.0,505.21,1000.0,1,"{'agent-0': 97.81, 'agent-1': 98.22, 'agent-2': 103.06, 'agent-3': 96.54, 'agent-4': 109.58}",{},0,"{'num_steps_trained': 121000, 'num_steps_sampled': 121000, 'wait_time_ms': 7.871, 'apply_time_ms': 7.358, 'dispatch_time_ms': 23.816, 'learner': {}}",2000,False,121000,118,3c53a64610424e268f2864e005a4de89,2020-11-25_16-46-52,1606319212,61,7.085901975631714,483.09068489074707,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa340eb4160>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62e80>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",483.09068489074707,121000,61
626.0,319.0,506.09,1000.0,1,"{'agent-0': 97.79, 'agent-1': 98.49, 'agent-2': 103.92, 'agent-3': 96.43, 'agent-4': 109.46}",{},0,"{'num_steps_trained': 123000, 'num_steps_sampled': 123000, 'wait_time_ms': 8.81, 'apply_time_ms': 8.081, 'dispatch_time_ms': 25.2, 'learner': {}}",2000,False,123000,119,3c53a64610424e268f2864e005a4de89,2020-11-25_16-47-00,1606319220,62,7.192832708358765,490.28351759910583,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87198>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87b00>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",490.28351759910583,123000,62
626.0,372.0,510.1,1000.0,4,"{'agent-0': 97.4, 'agent-1': 99.98, 'agent-2': 104.44, 'agent-3': 98.42, 'agent-4': 109.86}",{},0,"{'num_steps_trained': 125000, 'num_steps_sampled': 125000, 'wait_time_ms': 9.719, 'apply_time_ms': 7.787, 'dispatch_time_ms': 25.882, 'learner': {}}",2000,False,125000,123,3c53a64610424e268f2864e005a4de89,2020-11-25_16-47-07,1606319227,63,7.114505052566528,497.39802265167236,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d160>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62668>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",497.39802265167236,125000,63
626.0,372.0,511.38,1000.0,1,"{'agent-0': 97.7, 'agent-1': 100.36, 'agent-2': 104.59, 'agent-3': 98.96, 'agent-4': 109.77}",{},0,"{'num_steps_trained': 127000, 'num_steps_sampled': 127000, 'wait_time_ms': 9.131, 'apply_time_ms': 8.212, 'dispatch_time_ms': 23.55, 'learner': {}}",2000,False,127000,124,3c53a64610424e268f2864e005a4de89,2020-11-25_16-47-14,1606319234,64,7.103357315063477,504.50137996673584,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def753c8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87d30>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",504.50137996673584,127000,64
626.0,372.0,510.48,1000.0,1,"{'agent-0': 97.28, 'agent-1': 100.23, 'agent-2': 104.24, 'agent-3': 99.03, 'agent-4': 109.7}",{},0,"{'num_steps_trained': 129000, 'num_steps_sampled': 129000, 'wait_time_ms': 7.607, 'apply_time_ms': 8.491, 'dispatch_time_ms': 24.624, 'learner': {}}",2000,False,129000,125,3c53a64610424e268f2864e005a4de89,2020-11-25_16-47-21,1606319241,65,7.098558187484741,511.5999381542206,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d2e8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def624a8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",511.5999381542206,129000,65
626.0,372.0,511.93,1000.0,4,"{'agent-0': 97.41, 'agent-1': 100.35, 'agent-2': 104.93, 'agent-3': 99.87, 'agent-4': 109.37}",{},0,"{'num_steps_trained': 131000, 'num_steps_sampled': 131000, 'wait_time_ms': 8.75, 'apply_time_ms': 8.011, 'dispatch_time_ms': 24.783, 'learner': {}}",2000,False,131000,129,3c53a64610424e268f2864e005a4de89,2020-11-25_16-47-28,1606319248,66,7.136768102645874,518.7367062568665,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def756a0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87b70>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",518.7367062568665,131000,66
626.0,372.0,513.47,1000.0,1,"{'agent-0': 97.68, 'agent-1': 100.7, 'agent-2': 105.29, 'agent-3': 100.51, 'agent-4': 109.29}",{},0,"{'num_steps_trained': 133000, 'num_steps_sampled': 133000, 'wait_time_ms': 7.558, 'apply_time_ms': 7.956, 'dispatch_time_ms': 23.263, 'learner': {}}",2000,False,133000,130,3c53a64610424e268f2864e005a4de89,2020-11-25_16-47-35,1606319255,67,7.036715745925903,525.7734220027924,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8de48>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62390>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",525.7734220027924,133000,67
626.0,372.0,514.32,1000.0,1,"{'agent-0': 97.75, 'agent-1': 100.68, 'agent-2': 105.36, 'agent-3': 100.74, 'agent-4': 109.79}",{},0,"{'num_steps_trained': 135000, 'num_steps_sampled': 135000, 'wait_time_ms': 7.978, 'apply_time_ms': 7.878, 'dispatch_time_ms': 23.587, 'learner': {}}",2000,False,135000,131,3c53a64610424e268f2864e005a4de89,2020-11-25_16-47-42,1606319262,68,7.119751453399658,532.893173456192,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73b00>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62630>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",532.893173456192,135000,68
626.0,372.0,515.88,1000.0,4,"{'agent-0': 97.19, 'agent-1': 100.66, 'agent-2': 106.2, 'agent-3': 101.0, 'agent-4': 110.83}",{},0,"{'num_steps_trained': 137000, 'num_steps_sampled': 137000, 'wait_time_ms': 9.115, 'apply_time_ms': 8.044, 'dispatch_time_ms': 24.149, 'learner': {}}",2000,False,137000,135,3c53a64610424e268f2864e005a4de89,2020-11-25_16-47-50,1606319270,69,7.109781503677368,540.0029549598694,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d7f0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def628d0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",540.0029549598694,137000,69
626.0,372.0,515.24,1000.0,1,"{'agent-0': 96.76, 'agent-1': 100.32, 'agent-2': 106.34, 'agent-3': 100.97, 'agent-4': 110.85}",{},0,"{'num_steps_trained': 139000, 'num_steps_sampled': 139000, 'wait_time_ms': 11.012, 'apply_time_ms': 8.204, 'dispatch_time_ms': 24.166, 'learner': {}}",2000,False,139000,136,3c53a64610424e268f2864e005a4de89,2020-11-25_16-47-57,1606319277,70,7.142435312271118,547.1453902721405,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def739e8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87f28>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",547.1453902721405,139000,70
626.0,372.0,514.94,1000.0,1,"{'agent-0': 97.05, 'agent-1': 100.11, 'agent-2': 105.8, 'agent-3': 101.04, 'agent-4': 110.94}",{},0,"{'num_steps_trained': 141000, 'num_steps_sampled': 141000, 'wait_time_ms': 8.036, 'apply_time_ms': 8.082, 'dispatch_time_ms': 24.232, 'learner': {}}",2000,False,141000,137,3c53a64610424e268f2864e005a4de89,2020-11-25_16-48-04,1606319284,71,7.118685722351074,554.2640759944916,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d208>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d9b0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",554.2640759944916,141000,71
626.0,372.0,514.6,1000.0,4,"{'agent-0': 98.69, 'agent-1': 100.4, 'agent-2': 105.36, 'agent-3': 100.44, 'agent-4': 109.71}",{},0,"{'num_steps_trained': 143000, 'num_steps_sampled': 143000, 'wait_time_ms': 4.016, 'apply_time_ms': 8.315, 'dispatch_time_ms': 25.749, 'learner': {}}",2000,False,143000,141,3c53a64610424e268f2864e005a4de89,2020-11-25_16-48-11,1606319291,72,7.0528059005737305,561.3168818950653,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75630>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87828>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",561.3168818950653,143000,72
626.0,372.0,514.8,1000.0,1,"{'agent-0': 98.97, 'agent-1': 100.06, 'agent-2': 105.34, 'agent-3': 100.86, 'agent-4': 109.57}",{},0,"{'num_steps_trained': 145000, 'num_steps_sampled': 145000, 'wait_time_ms': 8.137, 'apply_time_ms': 8.178, 'dispatch_time_ms': 23.723, 'learner': {}}",2000,False,145000,142,3c53a64610424e268f2864e005a4de89,2020-11-25_16-48-18,1606319298,73,7.051841497421265,568.3687233924866,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62cf8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87c50>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",568.3687233924866,145000,73
626.0,372.0,514.55,1000.0,1,"{'agent-0': 99.04, 'agent-1': 99.66, 'agent-2': 105.29, 'agent-3': 100.89, 'agent-4': 109.67}",{},0,"{'num_steps_trained': 147000, 'num_steps_sampled': 147000, 'wait_time_ms': 6.42, 'apply_time_ms': 8.205, 'dispatch_time_ms': 25.393, 'learner': {}}",2000,False,147000,143,3c53a64610424e268f2864e005a4de89,2020-11-25_16-48-25,1606319305,74,7.214036703109741,575.5827600955963,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa340eb4160>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87da0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",575.5827600955963,147000,74
626.0,372.0,517.26,1000.0,4,"{'agent-0': 100.04, 'agent-1': 100.15, 'agent-2': 105.17, 'agent-3': 100.28, 'agent-4': 111.62}",{},0,"{'num_steps_trained': 149000, 'num_steps_sampled': 149000, 'wait_time_ms': 7.635, 'apply_time_ms': 8.012, 'dispatch_time_ms': 23.408, 'learner': {}}",2000,False,149000,147,3c53a64610424e268f2864e005a4de89,2020-11-25_16-48-32,1606319312,75,6.990210771560669,582.572970867157,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d400>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87c18>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",582.572970867157,149000,75
626.0,372.0,516.68,1000.0,1,"{'agent-0': 100.54, 'agent-1': 100.0, 'agent-2': 104.87, 'agent-3': 100.03, 'agent-4': 111.24}",{},0,"{'num_steps_trained': 151000, 'num_steps_sampled': 151000, 'wait_time_ms': 10.263, 'apply_time_ms': 8.07, 'dispatch_time_ms': 25.395, 'learner': {}}",2000,False,151000,148,3c53a64610424e268f2864e005a4de89,2020-11-25_16-48-39,1606319319,76,7.099279403686523,589.6722502708435,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75588>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def872b0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",589.6722502708435,151000,76
626.0,372.0,516.53,1000.0,1,"{'agent-0': 100.98, 'agent-1': 100.2, 'agent-2': 104.32, 'agent-3': 100.14, 'agent-4': 110.89}",{},0,"{'num_steps_trained': 153000, 'num_steps_sampled': 153000, 'wait_time_ms': 7.121, 'apply_time_ms': 8.32, 'dispatch_time_ms': 27.009, 'learner': {}}",2000,False,153000,149,3c53a64610424e268f2864e005a4de89,2020-11-25_16-48-47,1606319327,77,7.214529037475586,596.8867793083191,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def624a8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87a90>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",596.8867793083191,153000,77
626.0,388.0,517.3,1000.0,4,"{'agent-0': 100.54, 'agent-1': 100.49, 'agent-2': 104.22, 'agent-3': 100.23, 'agent-4': 111.82}",{},0,"{'num_steps_trained': 155000, 'num_steps_sampled': 155000, 'wait_time_ms': 43.595, 'apply_time_ms': 7.718, 'dispatch_time_ms': 26.341, 'learner': {}}",2000,False,155000,153,3c53a64610424e268f2864e005a4de89,2020-11-25_16-48-54,1606319334,78,7.385396242141724,604.2721755504608,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73ba8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def872e8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",604.2721755504608,155000,78
626.0,388.0,518.27,1000.0,1,"{'agent-0': 100.64, 'agent-1': 100.32, 'agent-2': 104.14, 'agent-3': 100.8, 'agent-4': 112.37}",{},0,"{'num_steps_trained': 157000, 'num_steps_sampled': 157000, 'wait_time_ms': 6.885, 'apply_time_ms': 8.518, 'dispatch_time_ms': 25.325, 'learner': {}}",2000,False,157000,154,3c53a64610424e268f2864e005a4de89,2020-11-25_16-49-01,1606319341,79,7.10385799407959,611.3760335445404,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62978>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87ac8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",611.3760335445404,157000,79
636.0,388.0,517.98,1000.0,3,"{'agent-0': 101.82, 'agent-1': 100.3, 'agent-2': 103.96, 'agent-3': 99.69, 'agent-4': 112.21}",{},0,"{'num_steps_trained': 159000, 'num_steps_sampled': 159000, 'wait_time_ms': 8.432, 'apply_time_ms': 8.005, 'dispatch_time_ms': 22.934, 'learner': {}}",2000,False,159000,157,3c53a64610424e268f2864e005a4de89,2020-11-25_16-49-08,1606319348,80,7.068211317062378,618.4442448616028,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def872e8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def874a8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",618.4442448616028,159000,80
636.0,388.0,517.87,1000.0,2,"{'agent-0': 102.82, 'agent-1': 100.18, 'agent-2': 103.26, 'agent-3': 99.56, 'agent-4': 112.05}",{},0,"{'num_steps_trained': 161000, 'num_steps_sampled': 161000, 'wait_time_ms': 8.189, 'apply_time_ms': 7.869, 'dispatch_time_ms': 24.122, 'learner': {}}",2000,False,161000,159,3c53a64610424e268f2864e005a4de89,2020-11-25_16-49-15,1606319355,81,7.083815097808838,625.5280599594116,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def37f28>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62c18>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",625.5280599594116,161000,81
636.0,388.0,517.64,1000.0,1,"{'agent-0': 103.24, 'agent-1': 99.69, 'agent-2': 102.93, 'agent-3': 99.31, 'agent-4': 112.47}",{},0,"{'num_steps_trained': 163000, 'num_steps_sampled': 163000, 'wait_time_ms': 8.648, 'apply_time_ms': 7.428, 'dispatch_time_ms': 24.115, 'learner': {}}",2000,False,163000,160,3c53a64610424e268f2864e005a4de89,2020-11-25_16-49-23,1606319363,82,7.083327293395996,632.6113872528076,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def871d0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87cf8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",632.6113872528076,163000,82
636.0,388.0,518.02,1000.0,3,"{'agent-0': 103.2, 'agent-1': 99.39, 'agent-2': 103.34, 'agent-3': 99.19, 'agent-4': 112.9}",{},0,"{'num_steps_trained': 165000, 'num_steps_sampled': 165000, 'wait_time_ms': 9.057, 'apply_time_ms': 7.485, 'dispatch_time_ms': 23.887, 'learner': {}}",2000,False,165000,163,3c53a64610424e268f2864e005a4de89,2020-11-25_16-49-30,1606319370,83,7.144495487213135,639.7558827400208,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62978>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87e80>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",639.7558827400208,165000,83
636.0,419.0,519.78,1000.0,2,"{'agent-0': 103.2, 'agent-1': 98.78, 'agent-2': 104.7, 'agent-3': 99.35, 'agent-4': 113.75}",{},0,"{'num_steps_trained': 167000, 'num_steps_sampled': 167000, 'wait_time_ms': 6.676, 'apply_time_ms': 7.006, 'dispatch_time_ms': 25.457, 'learner': {}}",2000,False,167000,165,3c53a64610424e268f2864e005a4de89,2020-11-25_16-49-37,1606319377,84,7.0892415046691895,646.8451242446899,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def875f8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def877f0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",646.8451242446899,167000,84
636.0,419.0,518.76,1000.0,1,"{'agent-0': 103.18, 'agent-1': 98.24, 'agent-2': 104.79, 'agent-3': 98.69, 'agent-4': 113.86}",{},0,"{'num_steps_trained': 169000, 'num_steps_sampled': 169000, 'wait_time_ms': 9.139, 'apply_time_ms': 7.609, 'dispatch_time_ms': 22.884, 'learner': {}}",2000,False,169000,166,3c53a64610424e268f2864e005a4de89,2020-11-25_16-49-44,1606319384,85,7.028403043746948,653.8735272884369,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75be0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75e48>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",653.8735272884369,169000,85
636.0,419.0,518.85,1000.0,3,"{'agent-0': 102.82, 'agent-1': 98.81, 'agent-2': 104.4, 'agent-3': 98.63, 'agent-4': 114.19}",{},0,"{'num_steps_trained': 171000, 'num_steps_sampled': 171000, 'wait_time_ms': 7.4, 'apply_time_ms': 8.818, 'dispatch_time_ms': 24.107, 'learner': {}}",2000,False,171000,169,3c53a64610424e268f2864e005a4de89,2020-11-25_16-49-51,1606319391,86,7.033712863922119,660.907240152359,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87240>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87e80>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",660.907240152359,171000,86
636.0,419.0,518.4,1000.0,1,"{'agent-0': 102.99, 'agent-1': 98.94, 'agent-2': 104.24, 'agent-3': 98.39, 'agent-4': 113.84}",{},0,"{'num_steps_trained': 173000, 'num_steps_sampled': 173000, 'wait_time_ms': 8.375, 'apply_time_ms': 7.425, 'dispatch_time_ms': 22.324, 'learner': {}}",2000,False,173000,170,3c53a64610424e268f2864e005a4de89,2020-11-25_16-49-58,1606319398,87,7.0140910148620605,667.9213311672211,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75da0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d128>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",667.9213311672211,173000,87
636.0,419.0,519.24,1000.0,2,"{'agent-0': 103.47, 'agent-1': 99.22, 'agent-2': 104.0, 'agent-3': 98.99, 'agent-4': 113.56}",{},0,"{'num_steps_trained': 175000, 'num_steps_sampled': 175000, 'wait_time_ms': 8.06, 'apply_time_ms': 8.22, 'dispatch_time_ms': 22.614, 'learner': {}}",2000,False,175000,172,3c53a64610424e268f2864e005a4de89,2020-11-25_16-50-05,1606319405,88,7.148318767547607,675.0696499347687,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def877f0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8dcf8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",675.0696499347687,175000,88
636.0,419.0,519.87,1000.0,3,"{'agent-0': 103.39, 'agent-1': 99.34, 'agent-2': 103.93, 'agent-3': 99.0, 'agent-4': 114.21}",{},0,"{'num_steps_trained': 177000, 'num_steps_sampled': 177000, 'wait_time_ms': 7.826, 'apply_time_ms': 7.716, 'dispatch_time_ms': 23.417, 'learner': {}}",2000,False,177000,175,3c53a64610424e268f2864e005a4de89,2020-11-25_16-50-12,1606319412,89,7.013501405715942,682.0831513404846,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def753c8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8dfd0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",682.0831513404846,177000,89
636.0,419.0,520.07,1000.0,1,"{'agent-0': 104.0, 'agent-1': 99.46, 'agent-2': 103.85, 'agent-3': 98.93, 'agent-4': 113.83}",{},0,"{'num_steps_trained': 179000, 'num_steps_sampled': 179000, 'wait_time_ms': 8.554, 'apply_time_ms': 7.443, 'dispatch_time_ms': 24.461, 'learner': {}}",2000,False,179000,176,3c53a64610424e268f2864e005a4de89,2020-11-25_16-50-19,1606319419,90,7.001103639602661,689.0842549800873,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73160>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62e10>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",689.0842549800873,179000,90
636.0,419.0,521.12,1000.0,2,"{'agent-0': 104.78, 'agent-1': 99.89, 'agent-2': 104.04, 'agent-3': 98.58, 'agent-4': 113.83}",{},0,"{'num_steps_trained': 181000, 'num_steps_sampled': 181000, 'wait_time_ms': 8.223, 'apply_time_ms': 7.079, 'dispatch_time_ms': 23.764, 'learner': {}}",2000,False,181000,178,3c53a64610424e268f2864e005a4de89,2020-11-25_16-50-26,1606319426,91,7.142856121063232,696.2271111011505,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8dfd0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8de10>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",696.2271111011505,181000,91
636.0,419.0,522.43,1000.0,3,"{'agent-0': 104.71, 'agent-1': 99.86, 'agent-2': 104.88, 'agent-3': 98.84, 'agent-4': 114.14}",{},0,"{'num_steps_trained': 183000, 'num_steps_sampled': 183000, 'wait_time_ms': 8.53, 'apply_time_ms': 7.122, 'dispatch_time_ms': 23.458, 'learner': {}}",2000,False,183000,181,3c53a64610424e268f2864e005a4de89,2020-11-25_16-50-33,1606319433,92,7.05537748336792,703.2824885845184,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87080>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62320>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",703.2824885845184,183000,92
636.0,419.0,522.44,1000.0,1,"{'agent-0': 105.31, 'agent-1': 99.97, 'agent-2': 104.12, 'agent-3': 98.99, 'agent-4': 114.05}",{},0,"{'num_steps_trained': 185000, 'num_steps_sampled': 185000, 'wait_time_ms': 6.631, 'apply_time_ms': 7.529, 'dispatch_time_ms': 24.404, 'learner': {}}",2000,False,185000,182,3c53a64610424e268f2864e005a4de89,2020-11-25_16-50-40,1606319440,93,7.02184796333313,710.3043365478516,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def755f8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75748>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",710.3043365478516,185000,93
636.0,419.0,522.16,1000.0,2,"{'agent-0': 105.57, 'agent-1': 99.74, 'agent-2': 104.51, 'agent-3': 98.62, 'agent-4': 113.72}",{},0,"{'num_steps_trained': 187000, 'num_steps_sampled': 187000, 'wait_time_ms': 6.84, 'apply_time_ms': 8.381, 'dispatch_time_ms': 23.498, 'learner': {}}",2000,False,187000,184,3c53a64610424e268f2864e005a4de89,2020-11-25_16-50-47,1606319447,94,6.982227325439453,717.286563873291,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def879b0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87208>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",717.286563873291,187000,94
636.0,419.0,523.93,1000.0,3,"{'agent-0': 106.59, 'agent-1': 99.41, 'agent-2': 104.76, 'agent-3': 98.67, 'agent-4': 114.5}",{},0,"{'num_steps_trained': 189000, 'num_steps_sampled': 189000, 'wait_time_ms': 7.218, 'apply_time_ms': 8.712, 'dispatch_time_ms': 23.072, 'learner': {}}",2000,False,189000,187,3c53a64610424e268f2864e005a4de89,2020-11-25_16-50-54,1606319454,95,7.00628924369812,724.2928531169891,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8dfd0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8de80>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",724.2928531169891,189000,95
636.0,419.0,524.39,1000.0,1,"{'agent-0': 106.57, 'agent-1': 99.42, 'agent-2': 105.56, 'agent-3': 98.12, 'agent-4': 114.72}",{},0,"{'num_steps_trained': 191000, 'num_steps_sampled': 191000, 'wait_time_ms': 4.858, 'apply_time_ms': 7.987, 'dispatch_time_ms': 22.843, 'learner': {}}",2000,False,191000,188,3c53a64610424e268f2864e005a4de89,2020-11-25_16-51-01,1606319461,96,6.9441845417022705,731.2370376586914,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d4e0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62ac8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",731.2370376586914,191000,96
636.0,419.0,523.54,1000.0,2,"{'agent-0': 106.7, 'agent-1': 99.62, 'agent-2': 105.17, 'agent-3': 97.5, 'agent-4': 114.55}",{},0,"{'num_steps_trained': 193000, 'num_steps_sampled': 193000, 'wait_time_ms': 8.912, 'apply_time_ms': 7.106, 'dispatch_time_ms': 23.746, 'learner': {}}",2000,False,193000,190,3c53a64610424e268f2864e005a4de89,2020-11-25_16-51-08,1606319468,97,6.930448293685913,738.1674859523773,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75080>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62978>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",738.1674859523773,193000,97
636.0,419.0,523.97,1000.0,3,"{'agent-0': 106.83, 'agent-1': 100.17, 'agent-2': 105.55, 'agent-3': 96.5, 'agent-4': 114.92}",{},0,"{'num_steps_trained': 195000, 'num_steps_sampled': 195000, 'wait_time_ms': 6.206, 'apply_time_ms': 7.196, 'dispatch_time_ms': 24.393, 'learner': {}}",2000,False,195000,193,3c53a64610424e268f2864e005a4de89,2020-11-25_16-51-15,1606319475,98,6.980390310287476,745.1478762626648,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def870f0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87f98>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",745.1478762626648,195000,98
636.0,419.0,524.14,1000.0,2,"{'agent-0': 106.86, 'agent-1': 100.43, 'agent-2': 104.38, 'agent-3': 97.43, 'agent-4': 115.04}",{},0,"{'num_steps_trained': 197000, 'num_steps_sampled': 197000, 'wait_time_ms': 8.083, 'apply_time_ms': 7.837, 'dispatch_time_ms': 24.328, 'learner': {}}",2000,False,197000,195,3c53a64610424e268f2864e005a4de89,2020-11-25_16-51-22,1606319482,99,7.012069225311279,752.1599454879761,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d400>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62cf8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",752.1599454879761,197000,99
636.0,419.0,524.91,1000.0,1,"{'agent-0': 107.25, 'agent-1': 100.69, 'agent-2': 104.33, 'agent-3': 97.46, 'agent-4': 115.18}",{},0,"{'num_steps_trained': 199000, 'num_steps_sampled': 199000, 'wait_time_ms': 9.524, 'apply_time_ms': 7.939, 'dispatch_time_ms': 21.666, 'learner': {}}",2000,False,199000,196,3c53a64610424e268f2864e005a4de89,2020-11-25_16-51-30,1606319490,100,7.13242506980896,759.292370557785,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62eb8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75d68>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",759.292370557785,199000,100
636.0,411.0,524.03,1000.0,3,"{'agent-0': 107.33, 'agent-1': 100.74, 'agent-2': 103.29, 'agent-3': 97.81, 'agent-4': 114.86}",{},0,"{'num_steps_trained': 201000, 'num_steps_sampled': 201000, 'wait_time_ms': 7.038, 'apply_time_ms': 7.505, 'dispatch_time_ms': 21.918, 'learner': {}}",2000,False,201000,199,3c53a64610424e268f2864e005a4de89,2020-11-25_16-51-37,1606319497,101,6.990160226821899,766.2825307846069,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75ac8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73320>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",766.2825307846069,201000,101
636.0,411.0,524.06,1000.0,2,"{'agent-0': 108.39, 'agent-1': 100.95, 'agent-2': 102.34, 'agent-3': 97.49, 'agent-4': 114.89}",{},0,"{'num_steps_trained': 203000, 'num_steps_sampled': 203000, 'wait_time_ms': 7.613, 'apply_time_ms': 7.302, 'dispatch_time_ms': 23.607, 'learner': {}}",2000,False,203000,201,3c53a64610424e268f2864e005a4de89,2020-11-25_16-51-44,1606319504,102,7.0352396965026855,773.3177704811096,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87ac8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d0b8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",773.3177704811096,203000,102
636.0,411.0,524.39,1000.0,1,"{'agent-0': 108.46, 'agent-1': 101.26, 'agent-2': 102.36, 'agent-3': 97.23, 'agent-4': 115.08}",{},0,"{'num_steps_trained': 205000, 'num_steps_sampled': 205000, 'wait_time_ms': 5.204, 'apply_time_ms': 8.403, 'dispatch_time_ms': 24.74, 'learner': {}}",2000,False,205000,202,3c53a64610424e268f2864e005a4de89,2020-11-25_16-51-51,1606319511,103,6.985867977142334,780.303638458252,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8d9e8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73160>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",780.303638458252,205000,103
636.0,411.0,523.3,1000.0,3,"{'agent-0': 109.7, 'agent-1': 101.11, 'agent-2': 101.34, 'agent-3': 96.61, 'agent-4': 114.54}",{},0,"{'num_steps_trained': 207000, 'num_steps_sampled': 207000, 'wait_time_ms': 5.635, 'apply_time_ms': 7.261, 'dispatch_time_ms': 23.793, 'learner': {}}",2000,False,207000,205,3c53a64610424e268f2864e005a4de89,2020-11-25_16-51-58,1606319518,104,6.996301651000977,787.2999401092529,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def875f8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87ba8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",787.2999401092529,207000,104
636.0,411.0,522.51,1000.0,1,"{'agent-0': 109.66, 'agent-1': 101.33, 'agent-2': 100.97, 'agent-3': 96.11, 'agent-4': 114.44}",{},0,"{'num_steps_trained': 209000, 'num_steps_sampled': 209000, 'wait_time_ms': 8.888, 'apply_time_ms': 7.678, 'dispatch_time_ms': 22.858, 'learner': {}}",2000,False,209000,206,3c53a64610424e268f2864e005a4de89,2020-11-25_16-52-05,1606319525,105,7.115656852722168,794.4155969619751,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75128>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def757b8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",794.4155969619751,209000,105
636.0,411.0,523.89,1000.0,2,"{'agent-0': 110.27, 'agent-1': 101.45, 'agent-2': 100.96, 'agent-3': 96.53, 'agent-4': 114.68}",{},0,"{'num_steps_trained': 211000, 'num_steps_sampled': 211000, 'wait_time_ms': 11.524, 'apply_time_ms': 7.507, 'dispatch_time_ms': 23.835, 'learner': {}}",2000,False,211000,208,3c53a64610424e268f2864e005a4de89,2020-11-25_16-52-12,1606319532,106,7.012854814529419,801.4284517765045,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87c88>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8df60>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",801.4284517765045,211000,106
636.0,411.0,526.26,1000.0,3,"{'agent-0': 110.77, 'agent-1': 101.62, 'agent-2': 101.26, 'agent-3': 96.88, 'agent-4': 115.73}",{},0,"{'num_steps_trained': 213000, 'num_steps_sampled': 213000, 'wait_time_ms': 9.162, 'apply_time_ms': 7.716, 'dispatch_time_ms': 23.11, 'learner': {}}",2000,False,213000,211,3c53a64610424e268f2864e005a4de89,2020-11-25_16-52-19,1606319539,107,7.077277421951294,808.5057291984558,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8de48>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73ef0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",808.5057291984558,213000,107
636.0,411.0,526.88,1000.0,1,"{'agent-0': 110.79, 'agent-1': 101.81, 'agent-2': 100.72, 'agent-3': 97.28, 'agent-4': 116.28}",{},0,"{'num_steps_trained': 215000, 'num_steps_sampled': 215000, 'wait_time_ms': 7.817, 'apply_time_ms': 7.262, 'dispatch_time_ms': 24.426, 'learner': {}}",2000,False,215000,212,3c53a64610424e268f2864e005a4de89,2020-11-25_16-52-26,1606319546,108,7.102491617202759,815.6082208156586,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87630>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def873c8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",815.6082208156586,215000,108
636.0,411.0,527.03,1000.0,2,"{'agent-0': 110.92, 'agent-1': 101.09, 'agent-2': 100.82, 'agent-3': 97.74, 'agent-4': 116.46}",{},0,"{'num_steps_trained': 217000, 'num_steps_sampled': 217000, 'wait_time_ms': 5.712, 'apply_time_ms': 8.282, 'dispatch_time_ms': 25.651, 'learner': {}}",2000,False,217000,214,3c53a64610424e268f2864e005a4de89,2020-11-25_16-52-33,1606319553,109,7.080581903457642,822.6888027191162,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75320>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75400>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",822.6888027191162,217000,109
636.0,411.0,527.91,1000.0,3,"{'agent-0': 111.63, 'agent-1': 101.35, 'agent-2': 100.55, 'agent-3': 97.63, 'agent-4': 116.75}",{},0,"{'num_steps_trained': 219000, 'num_steps_sampled': 219000, 'wait_time_ms': 7.601, 'apply_time_ms': 9.883, 'dispatch_time_ms': 23.4, 'learner': {}}",2000,False,219000,217,3c53a64610424e268f2864e005a4de89,2020-11-25_16-52-40,1606319560,110,6.955915212631226,829.6447179317474,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8de80>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def93a90>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",829.6447179317474,219000,110
636.0,411.0,527.46,1000.0,1,"{'agent-0': 111.4, 'agent-1': 101.1, 'agent-2': 100.48, 'agent-3': 97.67, 'agent-4': 116.81}",{},0,"{'num_steps_trained': 221000, 'num_steps_sampled': 221000, 'wait_time_ms': 7.968, 'apply_time_ms': 7.726, 'dispatch_time_ms': 24.783, 'learner': {}}",2000,False,221000,218,3c53a64610424e268f2864e005a4de89,2020-11-25_16-52-47,1606319567,111,6.993431091308594,836.638149023056,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73ef0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def93e48>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",836.638149023056,221000,111
636.0,411.0,526.43,1000.0,2,"{'agent-0': 111.75, 'agent-1': 101.09, 'agent-2': 99.57, 'agent-3': 96.78, 'agent-4': 117.24}",{},0,"{'num_steps_trained': 223000, 'num_steps_sampled': 223000, 'wait_time_ms': 8.719, 'apply_time_ms': 7.67, 'dispatch_time_ms': 23.862, 'learner': {}}",2000,False,223000,220,3c53a64610424e268f2864e005a4de89,2020-11-25_16-52-54,1606319574,112,7.049931049346924,843.688080072403,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75400>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def93ba8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",843.688080072403,223000,112
636.0,411.0,525.95,1000.0,3,"{'agent-0': 112.09, 'agent-1': 100.98, 'agent-2': 99.82, 'agent-3': 95.38, 'agent-4': 117.68}",{},0,"{'num_steps_trained': 225000, 'num_steps_sampled': 225000, 'wait_time_ms': 8.858, 'apply_time_ms': 7.482, 'dispatch_time_ms': 24.105, 'learner': {}}",2000,False,225000,223,3c53a64610424e268f2864e005a4de89,2020-11-25_16-53-01,1606319581,113,6.991374254226685,850.6794543266296,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73240>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def93cc0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",850.6794543266296,225000,113
636.0,411.0,526.21,1000.0,1,"{'agent-0': 112.49, 'agent-1': 101.14, 'agent-2': 99.66, 'agent-3': 94.8, 'agent-4': 118.12}",{},0,"{'num_steps_trained': 227000, 'num_steps_sampled': 227000, 'wait_time_ms': 7.757, 'apply_time_ms': 8.347, 'dispatch_time_ms': 22.868, 'learner': {}}",2000,False,227000,224,3c53a64610424e268f2864e005a4de89,2020-11-25_16-53-08,1606319588,114,7.076400518417358,857.755854845047,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8df60>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def937f0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",857.755854845047,227000,114
636.0,411.0,526.38,1000.0,2,"{'agent-0': 112.98, 'agent-1': 100.62, 'agent-2': 100.44, 'agent-3': 93.99, 'agent-4': 118.35}",{},0,"{'num_steps_trained': 229000, 'num_steps_sampled': 229000, 'wait_time_ms': 9.223, 'apply_time_ms': 7.409, 'dispatch_time_ms': 20.931, 'learner': {}}",2000,False,229000,226,3c53a64610424e268f2864e005a4de89,2020-11-25_16-53-16,1606319596,115,7.114952325820923,864.8708071708679,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75588>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def934e0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",864.8708071708679,229000,115
636.0,411.0,527.18,1000.0,3,"{'agent-0': 113.66, 'agent-1': 100.95, 'agent-2': 100.81, 'agent-3': 93.74, 'agent-4': 118.02}",{},0,"{'num_steps_trained': 231000, 'num_steps_sampled': 231000, 'wait_time_ms': 8.003, 'apply_time_ms': 6.733, 'dispatch_time_ms': 24.009, 'learner': {}}",2000,False,231000,229,3c53a64610424e268f2864e005a4de89,2020-11-25_16-53-23,1606319603,116,6.993304252624512,871.8641114234924,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def8de10>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def93cf8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",871.8641114234924,231000,116
636.0,411.0,526.96,1000.0,1,"{'agent-0': 113.91, 'agent-1': 100.63, 'agent-2': 100.92, 'agent-3': 93.4, 'agent-4': 118.1}",{},0,"{'num_steps_trained': 233000, 'num_steps_sampled': 233000, 'wait_time_ms': 8.013, 'apply_time_ms': 8.182, 'dispatch_time_ms': 24.974, 'learner': {}}",2000,False,233000,230,3c53a64610424e268f2864e005a4de89,2020-11-25_16-53-30,1606319610,117,7.233196973800659,879.0973083972931,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def93b00>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def933c8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",879.0973083972931,233000,117
636.0,411.0,526.76,1000.0,2,"{'agent-0': 114.54, 'agent-1': 100.39, 'agent-2': 100.72, 'agent-3': 93.43, 'agent-4': 117.68}",{},0,"{'num_steps_trained': 235000, 'num_steps_sampled': 235000, 'wait_time_ms': 6.416, 'apply_time_ms': 8.889, 'dispatch_time_ms': 24.46, 'learner': {}}",2000,False,235000,232,3c53a64610424e268f2864e005a4de89,2020-11-25_16-53-37,1606319617,118,7.5436272621154785,886.6409356594086,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def93eb8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87be0>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",886.6409356594086,235000,118
636.0,411.0,526.12,1000.0,3,"{'agent-0': 114.61, 'agent-1': 100.96, 'agent-2': 100.56, 'agent-3': 92.5, 'agent-4': 117.49}",{},0,"{'num_steps_trained': 237000, 'num_steps_sampled': 237000, 'wait_time_ms': 8.245, 'apply_time_ms': 8.465, 'dispatch_time_ms': 23.39, 'learner': {}}",2000,False,237000,235,3c53a64610424e268f2864e005a4de89,2020-11-25_16-53-44,1606319624,119,7.000113248825073,893.6410489082336,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def62cf8>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def73d30>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",893.6410489082336,237000,119
636.0,411.0,526.72,1000.0,1,"{'agent-0': 115.08, 'agent-1': 101.18, 'agent-2': 100.64, 'agent-3': 92.22, 'agent-4': 117.6}",{},0,"{'num_steps_trained': 239000, 'num_steps_sampled': 239000, 'wait_time_ms': 7.6, 'apply_time_ms': 7.228, 'dispatch_time_ms': 24.968, 'learner': {}}",2000,False,239000,236,3c53a64610424e268f2864e005a4de89,2020-11-25_16-53-52,1606319632,120,7.120419979095459,900.7614688873291,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def932b0>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def93b38>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",900.7614688873291,239000,120
636.0,411.0,526.24,1000.0,1,"{'agent-0': 115.39, 'agent-1': 100.9, 'agent-2': 100.42, 'agent-3': 92.01, 'agent-4': 117.52}",{},0,"{'num_steps_trained': 241000, 'num_steps_sampled': 241000, 'wait_time_ms': 7.42, 'apply_time_ms': 7.822, 'dispatch_time_ms': 22.215, 'learner': {}}",2000,False,241000,237,3c53a64610424e268f2864e005a4de89,2020-11-25_16-53-59,1606319639,121,7.054591655731201,907.8160605430603,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87518>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87f98>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",907.8160605430603,241000,121
636.0,411.0,525.85,1000.0,4,"{'agent-0': 114.96, 'agent-1': 100.3, 'agent-2': 99.87, 'agent-3': 91.43, 'agent-4': 119.29}",{},0,"{'num_steps_trained': 243000, 'num_steps_sampled': 243000, 'wait_time_ms': 6.581, 'apply_time_ms': 8.086, 'dispatch_time_ms': 22.981, 'learner': {}}",2000,False,243000,241,3c53a64610424e268f2864e005a4de89,2020-11-25_16-54-06,1606319646,122,7.050961256027222,914.8670217990875,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def75860>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def93940>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",914.8670217990875,243000,122
636.0,411.0,525.3,1000.0,1,"{'agent-0': 114.78, 'agent-1': 100.49, 'agent-2': 99.74, 'agent-3': 91.24, 'agent-4': 119.05}",{},0,"{'num_steps_trained': 245000, 'num_steps_sampled': 245000, 'wait_time_ms': 6.786, 'apply_time_ms': 7.189, 'dispatch_time_ms': 25.234, 'learner': {}}",2000,False,245000,242,3c53a64610424e268f2864e005a4de89,2020-11-25_16-54-13,1606319653,123,7.111002206802368,921.9780240058899,17798,jupyter-cuda-tf2,172.31.3.30,"{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None}, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': True, 'max_seq_len': 20, 'lstm_cell_size': 128, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'channel_major': False, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'conv_to_fc_net', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'env_config': {'func_create': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87f28>, 'env_name': 'harvest_env', 'run': 'A3C'}, 'env': 'harvest_env', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 0.5, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 10, 'train_batch_size': 30000, 'batch_mode': 'truncate_episodes', 'sample_async': True, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'input': 'sampler', 'input_evaluation': None, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policy_graphs': {'agent-0': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-1': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-2': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-3': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {}), 'agent-4': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>, Box(15, 15, 3), Discrete(8), {})}, 'policy_mapping_fn': <ray.tune.suggest.variant_generator.function object at 0x7fa2def87cf8>, 'policies_to_train': None}, 'use_pytorch': False, 'lambda': 1.0, 'grad_clip': 40.0, 'lr': 0.0001, 'lr_schedule': [[0, 0.00136], [20000000, 2.8e-05]], 'vf_loss_coeff': 0.5, 'entropy_coeff': -0.000687, 'min_iter_time_s': 5}",921.9780240058899,245000,123
